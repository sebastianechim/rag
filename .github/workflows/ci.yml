name: CI (Build & Test)

on:
  push:
    branches: [ main, pipelinewithoutgcp ]
  pull_request:
    branches: [ main, pipelinewithoutgcp ]

jobs:
  build-test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Ensure pip tools
        run: python -m pip install --upgrade pip setuptools wheel

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-cache-${{ runner.os }}-python-3.10
          restore-keys: |
            pip-cache-${{ runner.os }}-

      - name: Install runtime deps
        run: |
          python -m pip install -r requirements.txt
          python -m pip install -r requirements-dev.txt

      - name: Cache GGUF model
        id: cache-model
        uses: actions/cache@v4
        with:
          path: $HOME/.cache/llama_models
          key: gguf-model-${{ runner.os }}-gemma-270m-v1
          restore-keys: |
            gguf-model-${{ runner.os }}-gemma-270m-

      - name: Download GGUF model (if missing)
        env:
          HF_HUB_TOKEN: ${{ secrets.HF_HUB_TOKEN }}
        run: |
          mkdir -p $HOME/.cache/llama_models
          MODEL_PATH="$HOME/.cache/llama_models/gemma-270m.gguf"
          if [ -f "$MODEL_PATH" ]; then
            echo "Model already cached at $MODEL_PATH"
          else
            echo "Model not present - downloading..."
            python3 scripts/download_model.py \
            --url "https://huggingface.co/unsloth/gemma-3-270m-it-GGUF/resolve/main/gemma-3-270m-it-UD-Q8_K_XL.gguf" \
            --out "$HOME/.cache/llama_models/gemma-270m.gguf" \
            --token "${HF_HUB_TOKEN}" || true
          fi
          echo "Listing model dir:"
          ls -lh $HOME/.cache/llama_models || true
          echo "File size (bytes):"
          stat -c '%s %n' "$MODEL_PATH" || true

      - name: Lint (flake8)
        run: python -m flake8 src --max-line-length=100 || true

      - name: Check formatting (black)
        run: python -m black --check src || true

      - name: Run pytest
        run: |
          export CI_MODEL_PATH="$HOME/.cache/llama_models/gemma-270m.gguf"
          python -m pytest -q

      - name: Build docker
        run: docker build -t rag-serve:ci .